\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{upquote}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\newenvironment{statement}[2][Statement]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\usepackage{booktabs}
\usepackage{multirow}


% Listings package for code rendering (No external dependencies)
\usepackage{listings}  
\usepackage{xcolor}   % Color support
\usepackage{tcolorbox} % Box for better appearance

% Define custom colors for code highlighting
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstset{frame=tb,
    language=Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,       
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
}


\title{Handin 2}

\begin{document}
\maketitle

\section{Introduction}

We investigate the implementation and benchmarking of the Steepest Descent and Newton's Method, both using backtracking line search. We aim to analyze their convergence behavior and compare their efficiency.  We also solve theoretical problems that provide insights into the algorithmic choices and expected performance. 

In our benchmark protocol, we focus on the five functions provided in the case study. 

\section{Experiments}

\subsection{Experimental Setup: Stopping Criteria}
% Discuss what stopping criteria you use and why? How do you pick the values for the stopping thresholds?
We apply two kinds of stopping criteria, including the gradient norm threshold and the maximum iteration threshold. We use the gradient norm threshold to ensure the algorithm stops when the gradient is sufficiently small. We use the max iteration to limit the algorithm to run infinitely when there is ill conditions, in case the algorithm would run definitely.

\begin{lstlisting}
# Max Iteration Number
for _ in range(max_iter): 
    grad = grad_f(x)
    # Gradient Norm Threshold 
    if np.linalg.norm(grad) < tol:
        break
\end{lstlisting}

\textbf{Value Choice: }We choose the tolerance gradient value=1e-6 because it is small enough and will not cost too much iterations. We choose max iterations=1000, because the gradient norm threshold will reach firstly in our experiment, so the max iterations will not effect the gradient norm threshold. 

\subsection{Parameter Selection in Backtracking Line Search }
% How do you select parameter values for the backtracking line search? Is your result sensitive towards these settings?

As the Armijo issues, there are two selectable variables: The initial step size $\alpha$, the sufficient decrease parameter $c_1$, and the step shrinking factor$\rho$. 

\begin{equation}
\begin{aligned}
if \, & f(x_k + \alpha d_k) \leq f(x_k) + c_1 \alpha \nabla f(x_k)^T d_k:\\
& \alpha = \rho * \alpha
\end{aligned}
\end{equation}

Our initial setting for the three parameters is based on multiple tests of different values. % TODO

Sensitivity: % TODO

\section{Results and Discussion}

Our benchmark protocol cover the following three features: \\

(1) Different target functions, particularly the functions provided in the case\_study.py
(2) Different initial conditions(different $x_0$)\\
(3) Different backtracking step lengths(rho).\\ 


\subsection{Correctness Verification}
% Evaluate whether your results are correct. Describe the steps you took to ensure correctness of your algorithms and show data that supports your claims.

% TODO analysis f2 only

\subsection{Step Length Observations}
% For both algorithms, describe your observations of the chosen step lengths. Are there important differences between the functions and algorithms?

% TODO analysis f2 in detail only

Newton is not sensitive to the $\rho$ change.  % TODO reasoning

\subsection{Convergence Plots and Analysis}
% Show convergence plots and based on the empirical results, state whether the algorithms likely have Q-linear, Q-superlinear or maybe even Q-quadratic convergence on the different functions.



\section{Theory}

\subsection{Question 1: Sum of Different Powers Function}
% TODO jiayi

\subsection{Question 2: Sufficient Decrease Condition in 1D}
% TODO wendong

\section{Conclusion}


\end{document}
